# AI voice disorder prediction and explainability

- I’m interested in exploring the explainability of different machine learning model classes used in clinical decision making, particularly disease prediction. Medicine is a high-risk environment for AI applications, and understanding what goes into an automated system’s decision is key for accountability, building patient trust, and improving care in clinical workflows. In the field, models are commonly deployed in human-AI hybrid systems where clinicians have access to predictions but have the final say in decision making, and thus interpretability is an important factor in model development.
- For this project, I am working with [Bridge2AI-Voice](https://www.physionet.org/content/b2ai-voice/2.0.1/) dataset, which contains 19,271 voice recordings collected from 442 participants across five sites. There’s empirical evidence that voice can be used as a disease biomarker for various kinds of disorders: voice (e.g. laryngeal cancer), neurological (e.g. parkinson’s), mood and psychiatric (e.g. generalized anxiety disorder), and respiratory (e.g. asthma). This dataset contains various audio features from patient vocalization and speech, including spectrograms, Mel-frequency cepstral coefficients (MFCC’s), assorted acoustic features, and phonetic and prosodic features. It’s important to note that for this dataset, HIPAA Safe Harbor identifiers were removed, and we will only be working with audio features rather than raw voice/speech recordings directly.
- We will be applying machine learning methods to classify disease state, as well as applying explainability methods across model classes to derive insight into model decision making mechanisms. We will be focusing on vocal diseases, such as laryngeal cancer, benign cord lesions, recurrent laryngeal papilloma (RRP), spasmodic dysphonia, and unilateral vocal fold paralysis.

- References
    - Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D., Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926
    - Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D., Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P., Tang, J., Sigaras, A., Elemento, O., Dorr, D., … Bridge2AIVoice. (2024). Bridge2AI Voice REDCap (v3.23.0). Zenodo. https://zenodo.org/records/14989503
    - Florian Eyben, Martin Wöllmer, Björn Schuller: "openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor", Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010.
    - Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.
    - Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics. 2018 Nov 1;71:1-5.
    - Hwang, J., Hira, M., Chen, C., Zhang, X., Ni, Z., Sun, G., Ma, P., Huang, R., Pratap, V., Zhang, Y., Kumar, A., Yu, C.-Y., Zhu, C., Liu, C., Kahn, J., Ravanelli, M., Sun, P., Watanabe, S., Shi, Y., Tao, T., Scheibler, R., Cornell, S., Kim, S., & Petridis, S. (2023). TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch. arXiv preprint arXiv:2310.17864
    - Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-Bélair, V, & Shi, Y. (2021). TorchAudio: Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018.
    - Bevers, I., Ghosh, S., Johnson, A., Brito, R., Bedrick, S., Catania, F., & Ng, E. (2017). My Research Software (Version 1.3.0) [Computer software]. https://github.com/sensein/b2aiprep
    - Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84
